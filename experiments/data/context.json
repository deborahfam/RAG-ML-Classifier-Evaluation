[
  {
    "context_used": "One infers the presence of\ncertain objects from perceptual input, infers category membership from the perceived\nproperties of the objects, and then uses category information to make predictions about the\nobjects. For example, from its green and yellow mottled skin, one-foot diameter, ovoid\nshape, red flesh, black seeds, and presence in the fruit aisle, one can infer that an object is a\nwatermelon; from this, one infers that it would be useful for fruit salad.\n---\nSimilarly, in analyzing a stock market database \nto learn the target concept \"companies whose stock value will double over the \nnext 10 months,\" one might have approximate knowledge of economic causes \nand effects, suggesting that the gross revenue of the company is more likely to \nbe relevant than the color of the company logo. In both of these settings, our \nown prior knowledge is incomplete, but is clearly useful in helping discriminate \nrelevant features from irrelevant.\n---\nMost of the\nlinearly describable phenomena in nature are captured by multiple linear regression. For\nexample, the price of any item depends on the quantity being purchased, the time of the\nyear, and the number of items available in the inventory. For instance, the price of a bottle\nof wine depends primarily on how many bottles you bought. Also, the price is a bit higher\nduring festivals such as Christmas."
  },
  {
    "context_used": "6\nGrouping Datasets\nDuring data analysis, it is often essential to cluster or group data together based on certain\ncriteria. For example, an e-commerce store might want to group all the sales that were done\nduring the Christmas period or the orders that were received on Black Friday. These\ngrouping concepts occur in several parts of data analysis. In this chapter, we will cover the\nfundamentals of grouping techniques and how doing this can improve data analysis.\n---\nYou can’t do good work without a deep understanding of the context of what you’re doing. Why\nis your customer trying to solve this particular problem? What value will they derive from the\nsolution — how will your model be used, how will it fit into your customer’s business\nprocesses? What kind of data is available, or could be collected? What kind of machine learning\ntask can be mapped to the business problem?\n---\nFor example, a taxi driver usually won’t know whether a\nnew passenger prefers a leisurely or speedy journey, a cautious or aggressive driving style. A\nvirtual personal assistant starts out knowing nothing about the personal preferences of its"
  },
  {
    "context_used": "To illustrate, consider learning the target function \"people who plan to pur- \nchase new \nskis \nthis year,\" given a sample of training data collected by surveying \npeople as they arrive at a ski resort. In this case the instance space X is the space \nof all people, who might be described by attributes such as their age, occupation, \nhow many times they \nskied \nlast year, etc.\n---\nA dataset contains many observations about a particular\nobject. For instance, a dataset about patients in a hospital can contain many observations. A\npatient can be described by a patient identifier (ID), name, address, weight, date of birth, address,\nemail, and gender. Each of these features that describes a patient is a variable. Each\nobservation can have a specific value for each of these variables.\n---\nThis\nobjective should be correlated with your true goals, but usually will be distinct—the true\ngoal might be to maximize the number of users you gain and keep on your system, and the\nrevenue that they produce. Those are metrics you should track, but not necessarily ones that\nyou can directly build a machine learning model for."
  },
  {
    "context_used": "First ask, “what problem do I want to solve for my users?” An answer such as “make it easier\nfor users to organize and access their photos” is too vague; “help a user find all photos that\nmatch a specific term, such as Paris” is better.\n---\nAssistance games can be generalized to allow for multiple human participants, multiple\nrobots, imperfectly rational humans, humans who don’t know their own preferences, and so\non. By providing a factored or structured action space, as opposed to the simple atomic\nactions in the paperclip game, the opportunities for communication can be greatly\nenhanced.\n---\nThe movement addresses an\nissue that was recognized by Herbert Simon in 1971: “A wealth of information creates a\npoverty of attention.” Perhaps in the future we will have personal agents that stick up for\nour true long-term interests rather than the interests of the corporations whose apps\ncurrently fill our devices. It will be the agent’s job to mediate the offerings of various\nvendors, protect us from addictive attention-grabbers, and guide us towards the goals that\nreally matter to us."
  },
  {
    "context_used": "6\nGrouping Datasets\nDuring data analysis, it is often essential to cluster or group data together based on certain\ncriteria. For example, an e-commerce store might want to group all the sales that were done\nduring the Christmas period or the orders that were received on Black Friday. These\ngrouping concepts occur in several parts of data analysis. In this chapter, we will cover the\nfundamentals of grouping techniques and how doing this can improve data analysis.\n---\nIn the next chapter, we will continue more advanced descriptive statistics by using\ngrouping techniques. These grouping techniques are provided by the pandas library.\n---\nGrouping Datasets Chapter 6\n[ 164 ]\nUnderstanding groupby() \nDuring the data analysis phase, categorizing a dataset into multiple categories or groups is\noften essential. We can do such categorization using the pandas library. The pandas\ngroupby function is one of the most efficient and time-saving features for doing this."
  },
  {
    "context_used": "It minimizes the distance between the probability distributions output by the\nmodel and the true distribution of the targets.\n---\nTraveling salesperson problem (TSP)\nA VLSI layout problem requires positioning millions of components and connections on a\nchip to minimize area, minimize circuit delays, minimize stray capacitances, and maximize\nmanufacturing yield. The layout problem comes after the logical design phase and is usually\nsplit into two parts: cell layout and channel routing. In cell layout, the primitive\ncomponents of the circuit are grouped into cells, each of which performs some recognized\nfunction.\n---\nWe choose  different\nrandom projections and create  hash tables,  We then enter all the\nexamples into each hash table. Then when given a query point  we fetch the set of points\nin bin  of each hash table, and union these  sets together into a set of candidate\npoints,  Then we compute the actual distance to  for each of the points in  and return\nthe  closest points."
  },
  {
    "context_used": "Lang (1995) reports \nexperiments in which \nNEWSWEEDER \nused its learned profile of user interests to \nsuggest the most highly rated new articles each day. By presenting the user with \nthe top 10% of its automatically rated new articles each day, it created a pool of \narticles containing three to four times as many interesting articles as the general \npool of articles read by the user.\n---\nSearch (Chapters 3  to 5 ) and planning (Chapter 11) are the\nsubfields of AI devoted to finding action sequences that achieve the agent’s goals.\n---\nFirst ask, “what problem do I want to solve for my users?” An answer such as “make it easier\nfor users to organize and access their photos” is too vague; “help a user find all photos that\nmatch a specific term, such as Paris” is better."
  },
  {
    "context_used": "If you can train a stock market\nprediction model that makes you 10 on every trade, that’s great—but not if it costs you 20\nin computation cost for each prediction. A machine translation program that runs on your\nphone and allows you to read signs in a foreign city is helpful—but not if it runs down the\nbattery after an hour of use.\n---\nLikewise, the IMDB dataset you worked with was collected in 2011, and a\nmodel trained on it would likely not perform as well on reviews from 2020 compared to reviews\nfrom 2012, as vocabulary, expressions, and movie genres evolve over time. Concept drift is\nparticular acute in adversarial contexts like credit card fraud detection, where fraud patterns\nchange practically every day. Dealing with fast concept drift requires constant data collection,\nannotation, and model retraining.\n---\nWhen you click on something in an app, online game, social network, or\nshopping site, that serves as a recommendation that you (and your similar peers) would like\nto see similar things in the future. (Or it might be that the site is confusing and you clicked\non the wrong thing—the data are always noisy.) The feedback inherent in this system makes\nit very effective in the short run for picking out ever more addictive games and videos."
  },
  {
    "context_used": "It it is not just that the data will be changing—for example, new words will be used in spam\nemail messages. It is also that the entire data schema may change—you might start out\nclassifying spam email, and need to adapt to classify spam text messages, spam voice\nmessages, spam videos, etc. Figure 19.28 gives a general rubric to guide the practitioner in\nchoosing the appropriate level of testing and monitoring.\n---\nAs soon as you successfully classify a\nbatch of spam messages, the spammers will see what you have done and change their\ntactics, sending a new type of message you haven’t seen before. Non-spam also evolves, as\nusers change the mix of email versus messaging or desktop versus mobile services that they\nuse.\n---\nCommunication: This stage deals with disseminating the results to end\nstakeholders to use the result for business intelligence. One of the most notable\nsteps in this stage is data visualization. Visualization deals with information\nrelay techniques such as tables, charts, summary diagrams, and bar charts to\nshow the analyzed result. We will outline several visualization techniques in\nChapter 2, Visual Aids for EDA, with different types of data."
  },
  {
    "context_used": "This\nobjective should be correlated with your true goals, but usually will be distinct—the true\ngoal might be to maximize the number of users you gain and keep on your system, and the\nrevenue that they produce. Those are metrics you should track, but not necessarily ones that\nyou can directly build a machine learning model for.\n---\nYou’re not the first person to\ntry to build a spam detector, a music recommendation engine, or an image classifier.\n---\nThis is vital so that other potential readers can see and use\nyour unbiased opinion to make purchasing decisions, we can understand what our\ncustomers think about our products, and our authors can see your feedback on the title that\nthey have worked with Packt to create. It will only take a few minutes of your time, but is\nvaluable to other potential customers, our authors, and Packt. Thank you!"
  },
  {
    "context_used": "Traveling salesperson problem (TSP)\nA VLSI layout problem requires positioning millions of components and connections on a\nchip to minimize area, minimize circuit delays, minimize stray capacitances, and maximize\nmanufacturing yield. The layout problem comes after the logical design phase and is usually\nsplit into two parts: cell layout and channel routing. In cell layout, the primitive\ncomponents of the circuit are grouped into cells, each of which performs some recognized\nfunction.\n---\nA great deal\nof knowledge can be encoded, not just in the action sequences specified in each refinement\nbut also in the preconditions for the refinements. For some domains, HTN planners have\nbeen able to generate huge plans with very little search. For example, O-PLAN (Bell and Tate,\n1985), which combines HTN planning with scheduling, has been used to develop\nproduction plans for Hitachi.\n---\nThe key to HTN planning is a plan library containing known methods for implementing\ncomplex, high-level actions. One way to construct the library is to learn the methods from\nproblem-solving experience. After the excruciating experience of constructing a plan from\nscratch, the agent can save the plan in the library as a method for implementing the high-\nlevel action defined by the task."
  },
  {
    "context_used": "PREFACE \nThe field of machine learning is concerned with the question of how to construct \ncomputer programs that automatically improve with experience. In recent years \nmany successful machine learning applications have been developed, ranging from \ndata-mining programs that learn to detect fraudulent credit card transactions, to \ninformation-filtering systems that learn users' reading preferences, to autonomous \nvehicles that learn to drive on public highways.\n---\nMicrosoft’s AI for Humanitarian Action program\napplies AI to recovering from natural disasters, addressing the needs of children, protecting\nrefugees, and promoting human rights. Google’s AI for Social Good program supports work\non rainforest protection, human rights jurisprudence, pollution monitoring, measurement of\nfossil fuel emissions, crisis counseling, news fact checking, suicide prevention, recycling, and\nother issues.\n---\nSpend less time learning and more time coding with practical eBooks and Videos\nfrom over 4,000 industry professionals\nImprove your learning with Skill Plans built especially for you\nGet a free eBook or video every month\nFully searchable for easy access to vital information\nCopy and paste, print, and bookmark content\nDid you know that Packt offers eBook versions of every book published, with PDF and\nePub files available?"
  },
  {
    "context_used": "Attackers can use automation to probe for insecurities and they can apply\nreinforcement learning for phishing attempts and automated blackmail. Defenders can use\nunsupervised learning to detect anomalous incoming traffic patterns (Chandola et al., 2009;\nMalhotra et al.,2015) and various machine learning techniques to detect fraud (Fawcett and\nProvost, 1997; Bolton and Hand, 2002).\n---\nSURVEILLANCE AND PERSUASION: While it is expensive, tedious, and sometimes\nlegally questionable for security personnel to monitor phone lines, video camera feeds,\nemails, and other messaging channels, AI (speech recognition, computer vision, and\n \n---\nPerhaps your customer already has a hand-crafted\nalgorithm that handles spam filtering or credit card fraud detection — with lots of nested \n statements. Perhaps a human is currently in charge of manually handling the processif\nconsidered — monitoring the conveyor belt at the cookie plant and manually removing\nthe bad cookies, or crafting playlists of song recommendations to be sent out to users\nwho liked a specific artist."
  },
  {
    "context_used": "Perhaps your customer already has a hand-crafted\nalgorithm that handles spam filtering or credit card fraud detection — with lots of nested \n statements. Perhaps a human is currently in charge of manually handling the processif\nconsidered — monitoring the conveyor belt at the cookie plant and manually removing\nthe bad cookies, or crafting playlists of song recommendations to be sent out to users\nwho liked a specific artist.\n---\nIt is difficult to predict exact timelines for automation, but currently, and for the next few\nyears, the emphasis is on automation of structured analytical tasks, such as reading x-ray\nimages, customer relationship management (e.g., bots that automatically sort customer\ncomplaints and respond with suggested remedies), and business process automation that\ncombines text documents and structured data to make business decisions and improve\nworkflow.\n---\nLet's find the most active time of day for email\ncommunication. We can do that easily."
  },
  {
    "context_used": "But we really want to estimate the selling price of a house, not\nthe asking price. To solve this task we’ll need data on actual sales. But that doesn’t mean we\nshould throw away the data about asking price—we can use it as one of the input features.\n---\nFor example, medical researchers store\npatients' data, universities store students' and teachers' data, real estate industries store\nhouse and building datasets, and many more. A dataset contains many observations about\na particular object. Most of the datasets can be divided into numerical data and categorical\ndatasets. There are four types of data measurement scales: nominal, ordinal, interval, and\nratio.\n---\nunknown camera parameters (like focal length); and to exploit various sophisticated\nsearches for appropriate correspondences. It is practical to accurately reconstruct a model of\nan entire city from images. Some applications are:\nMODEL BUILDING: For example, one might build a modeling system that takes many\nviews depicting an object and produces a very detailed 3D mesh of textured polygons\nfor use in computer graphics and virtual reality applications."
  },
  {
    "context_used": "However, companies do respond to customers’ interests, and many customers have voiced\nthe opinion that they are interested in a fair and sustainable world. Tim O’Reilly explains\nwhy profit is not the only motive with the following analogy: “Money is like gasoline during\n\n\n---\nThe movement addresses an\nissue that was recognized by Herbert Simon in 1971: “A wealth of information creates a\npoverty of attention.” Perhaps in the future we will have personal agents that stick up for\nour true long-term interests rather than the interests of the corporations whose apps\ncurrently fill our devices. It will be the agent’s job to mediate the offerings of various\nvendors, protect us from addictive attention-grabbers, and guide us towards the goals that\nreally matter to us.\n---\nThis is roughly the situation of workers in a company, in which different\ndecision makers are pursuing, one hopes, the same goals on behalf of the company. The\nmain problem faced by the decision makers in this setting is the coordination problem:\nthey need to ensure that they are all pulling in the same direction, and not accidentally\nfouling up each other’s plans.",
    "query": "A marketing team seeks to understand different customer groups to tailor promotional strategies effectively."
  },
  {
    "context_used": "The simplest solution is to base the evaluation on\nthe average recommendation, perhaps with a variance determined by the number of\nrecommendations, but this fails to take into account the fact that some customers are kinder\nthan others and some are less honest than others.\n---\nSURVEILLANCE AND PERSUASION: While it is expensive, tedious, and sometimes\nlegally questionable for security personnel to monitor phone lines, video camera feeds,\nemails, and other messaging channels, AI (speech recognition, computer vision, and\n \n---\nPerhaps your customer already has a hand-crafted\nalgorithm that handles spam filtering or credit card fraud detection — with lots of nested \n statements. Perhaps a human is currently in charge of manually handling the processif\nconsidered — monitoring the conveyor belt at the cookie plant and manually removing\nthe bad cookies, or crafting playlists of song recommendations to be sent out to users\nwho liked a specific artist."
  },
  {
    "context_used": "natural language understanding) can be used in a scalable fashion to perform mass\nsurveillance of individuals and detect activities of interest. By tailoring information flows\nto individuals through social media, based on machine learning techniques, political\nbehavior can be modified and controlled to some extent—a concern that became\napparent in elections beginning in 2016.\n---\nThe system\nonly chooses to “buzz in” its answer if sufficiently conﬁdent it is correct. Similarly, Google has a\nsystem known as SmartASS (ad selection system) that predicts the probability you will click on\nan ad based on your search history and other user and ad-speciﬁc features (Metz 2010). This\nprobability is known as theclick-through rateor CTR, and can be used to maximize expected\nproﬁt. We will discuss some of the basic principles behind systems such as SmartASS later in\nthis book.\n---\nShared model\nWe expect that these models will improve over time, and that it will become unusual to start\na machine learning project from scratch, just as it is now unusual to do a Web development\nproject from scratch, with no libraries. It is possible that a big jump in model quality will\noccur when it becomes economical to process all the video on the Web; for example, the\nYouTube platform alone adds 300 hours of video every minute."
  },
  {
    "context_used": "However, companies do respond to customers’ interests, and many customers have voiced\nthe opinion that they are interested in a fair and sustainable world. Tim O’Reilly explains\nwhy profit is not the only motive with the following analogy: “Money is like gasoline during\n\n\n---\nThis\nobjective should be correlated with your true goals, but usually will be distinct—the true\ngoal might be to maximize the number of users you gain and keep on your system, and the\nrevenue that they produce. Those are metrics you should track, but not necessarily ones that\nyou can directly build a machine learning model for.\n---\nWhen you click on something in an app, online game, social network, or\nshopping site, that serves as a recommendation that you (and your similar peers) would like\nto see similar things in the future. (Or it might be that the site is confusing and you clicked\non the wrong thing—the data are always noisy.) The feedback inherent in this system makes\nit very effective in the short run for picking out ever more addictive games and videos."
  },
  {
    "context_used": "Exploratory Data Analysis Fundamentals Chapter 1\n[ 13 ]\nMaking sense of data\nIt is crucial to identify the type of data under analysis. In this section, we are going to learn\nabout different types of data that you can encounter during analysis. Different disciplines \nstore different kinds of data for different purposes. For example, medical researchers store\npatients' data, universities store students' and teachers' data, and real estate industries\nstorehouse and building datasets.\n---\nSurveillance camera\nAs more of our institutions operate online, we become more vulnerable to cybercrime\n(phishing, credit card fraud, botnets, ransomware) and cyberterrorism (including potentially\ndeadly attacks such as shutting down hospitals and power plants or commandeering self-\ndriving cars). Machine learning can be a powerful tool for both sides in the cybersecurity\nbattle.\n---\nIn \nthe field known as data mining, machine learning algorithms are being used rou- \ntinely to discover valuable knowledge from large commercial databases containing \nequipment maintenance records, loan applications, financial transactions, medical \nrecords, and the like. As our understanding of computers continues to mature, it"
  },
  {
    "context_used":"But we really want to estimate the selling price of a house, not\nthe asking price. To solve this task we’ll need data on actual sales. But that doesn’t mean we\nshould throw away the data about asking price—we can use it as one of the input features.\n---\nAlthough the second-price auction is truth-revealing, it turns out that auctioning  goods\nwith an  price auction is not truth-revealing. Many Internet search engines use a\nmechanism where they auction  slots for ads on a page. The highest bidder wins the top\nspot, the second highest gets the second spot, and so on. Each winner pays the price bid by\nthe next-lower bidder, with the understanding that payment is made only if the searcher\nactually clicks on the ad.\n---\nIn other cases, such as auctioning drilling rights for an oil tract, the item has a common\nvalue—the tract will produce some amount of money,  and all bidders value a dollar\nequally—but there is uncertainty as to what the actual value of  is. Different bidders have\ndifferent information, and hence different estimates of the item’s true value. In either case,\nbidders end up with their own  Given  each bidder gets a chance, at the appropriate\ni vi\nX,\nX\nvi. vi,"
  },
  {
    "context_used":"Now suppose that a seismologist offers the company the results of a survey of block number\n3, which indicates definitively whether the block contains oil. How much should the\ncompany be willing to pay for the information? The way to answer this question is to\nexamine what the company would do if it had the information:\nWith probability , the survey will indicate oil in block 3. In this case, the company\nwill buy block 3 for  dollars and make a profit of  dollars.\n---\nThe sounding observation can be in one of 3 states:s =0 is\na diffuse reﬂection pattern, suggesting no oil;s =1 is an open reﬂection pattern, suggesting\nsome oil; ands =2 is a closed reﬂection pattern, indicating lots of oil. SinceS is caused byO,\nwe add anO → S arc to our model. In addition, we assume that the outcome of the sounding\ntest will be available before we decide whether to drill or not; hence we add aninformation\narc from S to D. This is illustrated in Figure 10.12(b).\n---\n15.5. GP latent variable model 541\n0 0.05 0.1 0.15 0.2\n0\n.05\n0.1\n.15\n0.2\n.25\n0.3\n(a)\n−0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6\n0.6\n0.4\n0.2\n0\n0.2\n0.4\n(b)\nFigure 15.10 2d representation of 12 dimensional oil ﬂow data. The different colors/symbols represent\nthe 3 phases of oil ﬂow. (a) Kernel PCA with Gaussian kernel. (b) GP-LVM with Gaussian kernel. The\nshading represents the precision of the posterior, where lighter pixels have higher precision. From Figure 1\nof (Lawrence 2005)."
  },
  {
    "context_used":"The movement addresses an\nissue that was recognized by Herbert Simon in 1971: “A wealth of information creates a\npoverty of attention.” Perhaps in the future we will have personal agents that stick up for\nour true long-term interests rather than the interests of the corporations whose apps\ncurrently fill our devices. It will be the agent’s job to mediate the offerings of various\nvendors, protect us from addictive attention-grabbers, and guide us towards the goals that\nreally matter to us.\n---\nThis\nobjective should be correlated with your true goals, but usually will be distinct—the true\ngoal might be to maximize the number of users you gain and keep on your system, and the\nrevenue that they produce. Those are metrics you should track, but not necessarily ones that\nyou can directly build a machine learning model for.\n---\nToday, large companies work with image datasets, video datasets, and\nnatural-language datasets that couldn’t have been collected without the internet. User-generated\nimage tags on Flickr, for instance, have been a treasure trove of data for computer vision. So are\nYouTube videos. And Wikipedia is a key dataset for natural-language processing."
  },
  {
    "context_used":"For example, in \nanalyzing a database of medical records to learn \"symptoms for which treatment \nx is more effective than treatment y,\" one often begins with approximate prior \nknowledge \n(e.g., \na qualitative model of the cause-effect mechanisms underlying \nthe disease) that suggests the patient's temperature is more likely to be relevant \nthan the patient's middle initial.\n---\nFor a doctor does not deliberate whether he shall\nheal, nor an orator whether he shall persuade, … They assume the end and consider how and by\nwhat means it is attained, and if it seems easily and best produced thereby; while if it is achieved by\none means only they consider how it will be achieved by this and by what means this will be\nachieved, till they come to the first cause, … and what is last in the order of analysis seems to be first\nin the order of becoming.\n---\nFor example, given a particular patient, a frequentist who wants to estimate the probability\nof a cavity will consider a reference class of other patients who are similar in important ways\n—age, symptoms, diet—and see what proportion of them had a cavity. If the dentist\nconsiders everything that is known about the patient—hair color, weight to the nearest\ngram, mother’s maiden name—then the reference class becomes empty. This has been a\nvexing problem in the philosophy of science."
  },
  {
    "context_used":"Uncertainty about one’s own preferences\nImagine that you are at an ice-cream shop in Thailand and they have only two flavors left:\nvanilla and durian. Both cost . You know you have a moderate liking for vanilla and you’d\nbe willing to pay up to  for a vanilla ice cream on such a hot day, so there is a net gain of \n for choosing vanilla.\n---\nFor example, future truck drivers\nwill spend less time holding the steering wheel and more time making sure that the goods\nare picked up and delivered properly; serving as customer service representatives and\nsalespeople at either end of the journey; and perhaps managing convoys of, say, three\nrobotic trucks.\n---\nMost of the\nlinearly describable phenomena in nature are captured by multiple linear regression. For\nexample, the price of any item depends on the quantity being purchased, the time of the\nyear, and the number of items available in the inventory. For instance, the price of a bottle\nof wine depends primarily on how many bottles you bought. Also, the price is a bit higher\nduring festivals such as Christmas."
  },
  {
    "context_used":"Traveling salesperson problem (TSP)\nA VLSI layout problem requires positioning millions of components and connections on a\nchip to minimize area, minimize circuit delays, minimize stray capacitances, and maximize\nmanufacturing yield. The layout problem comes after the logical design phase and is usually\nsplit into two parts: cell layout and channel routing. In cell layout, the primitive\ncomponents of the circuit are grouped into cells, each of which performs some recognized\nfunction.\n---\nThis is roughly the situation of workers in a company, in which different\ndecision makers are pursuing, one hopes, the same goals on behalf of the company. The\nmain problem faced by the decision makers in this setting is the coordination problem:\nthey need to ensure that they are all pulling in the same direction, and not accidentally\nfouling up each other’s plans.\n---\nThe representation of resources as numerical quantities, such as Inspectors(2), rather than as\nnamed entities, such as  and , is an example of a technique called\naggregation: grouping individual objects into quantities when the objects are all\nindistinguishable. In our assembly problem, it does not matter which inspector inspects the\ncar, so there is no need to make the distinction. Aggregation is essential for reducing\ncomplexity."
  },
  {
    "context_used":"Scheduling\nThe real world also imposes resource constraints: an airline has a limited number of staff,\nand staff who are on one flight cannot be on another at the same time. This section\nintroduces techniques for planning and scheduling problems with resource constraints.\n---\nPerhaps your customer already has a hand-crafted\nalgorithm that handles spam filtering or credit card fraud detection — with lots of nested \n statements. Perhaps a human is currently in charge of manually handling the processif\nconsidered — monitoring the conveyor belt at the cookie plant and manually removing\nthe bad cookies, or crafting playlists of song recommendations to be sent out to users\nwho liked a specific artist.\n---\n(It is up to the\ncontingent-planning algorithm to make sure that the agent will never end up in a belief state\nwhere the condition formula’s truth value is unknown.) Note that with first-order\nconditions, the formula may be satisfied in more than one way; for example, the condition \n might be satisfied by  and by  if both\ncans are the same color as the table. In that case, the agent can choose any satisfying\nsubstitution to apply to the rest of the plan."
  },
  {
    "context_used":"To illustrate, imagine some new change in the environment of some species, \nsuch as a new predator. Such a change will selectively favor individuals capa- \nble of learning to avoid the predator. As the proportion of such self-improving \nindividuals in the population grows, the population will be able to support a \nmore diverse gene pool, allowing evolutionary processes (even \nnon-Lamarckian \ngenerate-and-test processes) to adapt more rapidly.\n---\nClustering\nlives in lakes\nis an amphibian\nis a rodent\nis tall\nis a fish\nis slimy\nhas horns\nhas hooves\nis a feline\nroars\nhas fins\nhas webbed feet\neats nuts\nis smooth\nlives in trees\nis large\nlives in cold climates\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nis ferocious\nis dangerous\nis a carnivore\nis a predator\nlives in water\nflies\nis long\neats leaves\neats animals\nlives in grass\neats fish\nlives in hot climates\nLeopard\nAlligator\nPython\nSeal\nDolphin\nFrog\nJellyfish\nOctopus\nPenguin\nFinch\nSeagull\nOwl\nEagle\nDragonfly\nBat\nGrasshopper\nAnt\nBee\nSheep\nMonkey\nIguana\nOstrich\nhas bones\nlays eggs\nis warm−blooded\nis a mammal\nsquawks\nhas a beak\nhas a tongue\nis green\nhas a spinal cord\nis a lizard\nhas antennae\nhas flippers\nhas paws\nhas a large brain\nhas a tail\nis furry\neats mice\neats rodents\nhas a snout\nis brown\nmakes loud noises\nhas teeth\nhas feet\nis smart\ntravels in groups\nLeopard\nSheep\nSeal\nDolphin\nMonkey\nBat\nAlligator\nIguana\nFrog\nPython\nFinch\nOstrich\nSeagull\nOwl\nPenguin\nEagle\nGrasshopper\nAnt\nBee\nJellyfish\nOctopus\nDragonfly\nA BC\nFrog\nFigure 25.20 MAP estimate produced by the crosscat system when applied to a binary data matrix of\nanimals (rows) by features (columns).\n---\n268 MACHINE LEARNING \nwas allowed, the population failed to improve its fitness over time. However, \nwhen individual learning was allowed, the population quickly improved its fit- \nness. During early generations of evolution the population contained a greater \nproportion of individuals with many trainable weights."
  },
  {
    "context_used":"Moreover, there are many outliers associated with electricity\nconsumption, wind production, and solar production.\n---\nTime Series Analysis Chapter 8\n[ 227 ]\nThe output of the preceding code is given here:\nAs shown in the preceding graph, electricity consumption is higher on weekdays and\nlowest at the weekends. We can see the consumption for each day of the month. We can\nzoom in further to see how consumption plays out in the last week of December.\n---\nIt describes\ndistributions over the number of events in a given time interval (most of which are naturally\noccurring) as well as over their time, magnitude, depth, and location. The locations of\nnatural events are distributed according to a spatial prior that is trained (like other parts of\nthe model) from historical data; man-made events are, by the treaty rules, assumed to occur\nuniformly over the surface of the Earth."
  },
  {
    "context_used":"At this point if we asked the model which next word was more probable, “compete” or\n“competes,” we would expect it to pick “compete” because it agrees with the subject “The\nathletes.” An LSTM can learn to create a latent feature for the subject person and number\nand copy that feature forward without alteration until it is needed to make a choice like this.\n---\nWhen this\nhappens, the optimum decision rule can be stated very simply: to classify a feature\nvectorx, measure the Euclidean distance ∥x − µi∥ from each x to each of the c\nmean vectors, and assignx to the category of the nearest mean. Such a classiﬁer is\ncalled a minimum distance classiﬁer.\n---\nIf \nthe task is to learn classification rules, then the fitness function typically has a \ncomponent that scores the classification accuracy of the rule over a set of provided \ntraining examples. Often other criteria may be included as well, such as the com- \nplexity or generality of the rule."
  }
  
]